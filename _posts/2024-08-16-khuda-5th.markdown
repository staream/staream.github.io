---
layout: post
title: 	"5th-week ML"
date: 	2024-07-26 15:52:17 +0900
categories: KhuDa
---
# 05. 트리알고리즘

# 05-1 결정트리

## 기존의 모델 설명
다른 특성을 이용하여 이 객체가 어디에 속하는지 알 수 있니??

**로지스틱 회귀로 와인 분류하기**

와인 데이터 가져오기

```python
import pandas as pd
wine = pd.read_csv('http://bit.ly/wine-date')

wine.head() # 와인 데이터 셋 불러오기
```
![alt text](image-39.png)

해당 이미지의 타깃(class 열)값이 0(음)이면 red, 1(양)이면 white이다.

1. info()
```python
wine.info()
```

해당 메소드는 데이터프레임의 각 열의 데이터 타입과 누락된 데이터가 있는지 확인하는 데 유용합니다.

![alt text](image-40.png)

출력 결과를 보면 총 6,497개의 샘플이 있고 4개의 열은 모두 실숫값이다. **Non-Null Count**가 모두 6497이므로 **누락**은 없다.

누락이 있는 경우???

- 그 데이터를 버리기
- 누락된 곳을 평균으로 채우고 사용하기.

이것은 사용자의 선택에 따라 달라진다.

2. describe()

열에 대한 간략한 통계를 출력해줍니다.

(예시) 최소, 최대, 평균값 등을 볼 수 있다.

```python
wine.describe()
```

![alt text](image-41.png)

위에서 볼 수 있듯이

| 한글 | 영어 | 
| --- | --- | 
| 평균 | mean |
| 표준편차 | std |
| 최소 | min |
| 1사분위 | 25% |
| 중간값/2사분위 | 50% |
| 3사분위 | 75% |
| 최대 | max |

위의 사진을 통해 **추론**할 수 있는 사실

- 스케일 - 알콜, 당도, pH값의 스케일이 다르다.

이에 대한 해결법
1) 넘파이로 변환
```python
data = wine[['alcohol','sugar','pH']].to_numpy()
target = wine['class'].to_numpy()
```
2) 데이터 셋 분할
```python
from sklearn.model_selection import train_test_split
train_input, test_input, train_target, test_target = train_test_split(data, target, test_size= 0.2, random_state=42)

print(train_input.shape, test_input.shape)
# (5197,3) (1300,3)
```
3) train, test 변환
```python
from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
ss.fit(train_input)
train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)
```

4) logistic regression 해보기
```python
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()
lr.fit(train_scaled, train_target)
print(lr.score(train_scaled, train_target))
print(lr.score(test_scaled, test_target))
# 0.78
# 0.77
```

결과가 너무 만족스럽지 않다.

이를 통해 과소적합이 되었다는 것을 알 수 있다.

이를 해결할 수 있는 방법

1. C : 규제 강도

2. solver : 다른 알고리즘

3. 다항 특성을 추가하는 방안

이에 대한 매개변수는 **197쪽**을 참조하기

coef_, intercept_

**이에 대해 우리는 쉽게 납득할 수 있을까????**

## 결정 트리 Desicion Tree
사용하는 이유 : "이유를 설명하기 쉽다."

이전엔 linear_model이었지만 이제는 tree를 입력한다.

객체는 DesicionTreeClassifier로 부른다.

```python
from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier(random_state=42)
dt.fit(train_scaled,train_target)
print(dt.score(train_scaled, train_target))
print(dt.score(test_scaled, test_target))
#0.99
#0.85
```

테스트 성능이 조금 낮기에 **과대적합**된 모델이라고 볼 수 있다.

그림으론 어떻게 표현할 수 있을까요?

이용함수는 plot_tree() 함수를 사용하면 된다.

```python
import matplot.pyplot as plt
from sklearn.tree import plot_tree
plt.figure(figsize=(10,7))
plot_tree(dt)
plt.show()
```
![alt text](image-42.png)

각 용어

맨 위의 노드 : root node

맨 아래 끝에 달린 노드 : leaf node

노드 : 훈련 데이터의 특성에 대한 테스트

특성에 대한 데이터를 기준에 테스트해보는 것이다.

plot_tree() 함수의 트리의 깊이를 **제한**하여 출력하는 방법

매개변수

max_depth : 1이면 leaf를 제외한 하나의 노드가 생긴다. 하지만 root는 포함한다.

filled : true를 넣으면 클래스에 맞게 노드의 색을 칠한다.

feature_names : 특성의 이름을 전달하는 목적, 이를 통해 노드가 어떤 특성을 테스트를 받는지 알 수 있다.
```python
plt.figure(figsize=(10,7))
plot_tree(dt,max_depth=1, filled=True, feature_names =['alcohol', 'sugar', 'pH'])
plt.show()
```
![alt text](image-43.png)

![alt text](image-44.png)

위의 이미지는 feature_names를 쓰고나서 나타나는 문자에 대한 내용들이다.

gini는 불순도이다.

samples는 샘플의 개수이다.

value는 [음성 클래스, 양성 클래스]

판단 기준 : value에서 가장 많은 값을 가진 클래스가 **예측 클래스**이다.

- **불순도**
    - gini는 "gini impurity"를 의미한다.
    - DecisionTreeClassifier 클래스의 criterion 매개변수의 기본값은 'gini'이다.
    - 계산법
        - gini : 1- ((음성 클래스 비율)^2+(양성 클래스 비율)^2)
    - 해석
        - gini : 0.5는 최악이다.
            - 0이면 최고이다.
    - 목표 
        - 부모 노드와 자식 노드 간의 불순도 차이가 크도록 트리를 키운다.

    ![alt text](image-45.png)

    - 정보 이득 **information gain**
        - 불순도의 차이를 말하는 단어이다.
    
    - **entropy**
    - DecisionTreeClassifier 클래스의 criterion 매개변수의 값으로 entropy를 지정하여 사용한다.
    - 계산법은 제곱이 아닌 **밑이 2인 log**를 사용하여 곱한다.

    ![alt text](image-46.png)

이 트리는 제한 없이 자라나기 때문에 위에서 0.99와 0.85로 정확도가 달랐던 이유가 여기서 나온다.

- **가지치기**
    - 방법
    1) 최대 깊이 지정하기
        - max_depth를 3으로 지정하여 모델을 만들어보기
         ```python
        dt = DecisionTreeClassifier(max_depth=3, random_state=42)
        dt.fit(train_scaled, train_target)
        print(dt.score(train_scaled, train_target))
        print(dt.score(test_scaled, test_target))
        #0.84
        #0.84
        ```
        
        모델에 성능을 테스트는 거의 그대로이다.

        ```python
        plt.figure(figsize=(10,7))
        plot_tree(dt,max_depth=1, filled=True, feature_names =['alcohol', 'sugar', 'pH'])
        plt.show()
        ```

        ![alt text](image-47.png)

- 트리 알고리즘의 장점
    1) 전처리
    - 트리는 불순도를 기준으로 샘플을 나누기에 불순도는 클래스별 비율을 가지고 계산한다.
    
    이는 **전처리 과정**을 생략해도 된다.

    - 전처리 전과의 차이는 기준선에 들어가는 값의 크기이다.
 
    2) 특성의 중요도
    - 특성의 중요도를 찾는 방법

    feature_importances_

    ```python
    print(dt.feature_importances_)
    # [0.123456 0.86862934 0.0079144]
    # 총 합이 1이 된다.
    ```

# 05-2 교차 검증과 그리드 서치
검증이 나온 계기

모델의 일반화 능력을 증가시키기 위해 해당 모델의 **hyper parameter**를 바꾼 형태의 다수 모델들을 이용할 경우엔 이를 **테스트에 맞춘 과적합**이라고 생각할 수 있지 않을까?

이전까지는 문제를 간단히 하려고 테스트 세트를 사용하면서 정확도를 통해 일반화 성능을 측정했다. 하지만 이는 옳지 않다. 딱 한 번만 사용하는 것이 좋다.

validation set(검증세트)

![alt text](image-48.png)

보통은 20~30%를 테스트 세트와 훈현 세트로 나눈다.

## 검증
**Process using validation** 
1) 모델 만들기

테스트하고 싶은 매개변수를 바꿔가며 가장 좋은 모델을 고른다.

2) 재훈련 

매개변수를 사용해 훈련 세트와 검증 세트를 거쳐 전체 훈련 데이터에서 훈련을 다시 한다.

3) 모델 평가

테스트를 통한 평가에서 나온 점수로 모델을 평가한다.

```python
import pandas as pd
wine = pd.read_csv("http://bit.ly/wine-date')
```

**class와 feature 열을 나누기**
```python
data = wine[['alcohol', 'sugar','pH']].to_numpy()
target = wine['class'].to_numpy()
```

**Divide train, test**
```python
from sklearn.model_selection import train_test_split
train_input, test_input, train_target, test_target = train_test_split(data, target, test_size =0.2, random_state=42)
```

**Divide train, validation**
```python
sub_input, val_input, sub_target, val_target = train_test_split(train_input, train_target, test_size =0.2, random_state=42)
```
```python
print(sub_input.shape)
#(4157,3)
```

**Train model**
```python
from sklearn.tree import DecisionTreeClassifier
dt= DecisionTreeClassifier(random_state=42)
dt.fit(sub_input,sub_target)
print(dt.score(sub_input, sub_target))
print(dt.score(val_input, val_target))
#0,96
#0.86
```

## 교차 검증
**교차 검증**(cross validation)

핵심 개념
- **검증 세트를 떼어** 내어 평가하는 과정을 여러 번 반복한다.
- 이 점수를 평균하여 최종 검증 점수를 얻는다.

![alt text](image-49.png)

이 과정을 k-fold cross validation이라고 한다.

훈련 세트를 몇 부분 나누냐에 따라 k가 결정된다.

사용 방법
- cross_validate()

default 값은 k=5이다.

cv 매개변수가 k값을 바꿀 수 있다.

```python
from sklearn.model_selection import cross_validate
scores = cross_validate(dt, train_input, train_target)
print(scores)
# dict
# fit_time : 훈련 시간
# score_time : 검증 시간
# test_score : 각 데이터로 학습된 모델의 검증 세트를 지날 때의 평가.
#

import numpy as np
print(np.mean(scores['test_score']))
#0.855
```

cv 매개변수의 다른 역할로는 데이터 분할을 할 때에도 사용된다. 이를 위해선 **분할기** splitter를 지정해야 한다.

각각 데이터를 골고루 섞기 위해 하는 작업

회귀 : KFold

분류 : StratifiedKFold

```python
from sklearn.model_selection import StratifiedKFold
scores = cross_validate(dt, train_input, train_target, cv = StratifiedKFold())
print(np.mean(scores['test_score']))
# 0.855
```

10-fold cross validation 의 경우

```python
from sklearn.model_selection import StratifiedKFold

splitter = StratifiedKFold(n_splits=10, shuffle =True, random_state =42)

scores = cross_validate(dt, train_input, train_target, cv = splitter)
print(np.mean(scores['test_score']))
# 0.857
```

## 하이퍼파라미터 튜닝
목표 : 좋은 성능을 가진 모델을 찾아보자

모델 파라미터

하이퍼파라미터

주의점은 다수의 하이퍼파라미터는 하나만 먼저 정한다는 식이 아닌, 모든 파라미터를 변화시켜주면서 최적값을 찾아야 한다.

분류에선 max_depth, min_samples_split을 동시에 바꾸면서 찾아야 한다.

**찾는 방법**

for문을 대신할 scikitlearn에서 제공하는 **Grid Search**가 있다.

라이브러리 GridSearchCV 클래스

장점
1) cross_validate() 함수를 호출할 필요가 없다.

**GridSearchCV를 통한 하이퍼파라미터 탐색과 교차검증**

최적값을 찾을 파라미터 : min_impurity_decrease
```python
from sklearn.model_selection import GridSearchCV
params = {'min_impurity_decrease' : {0.0001,0.0002,0.0003,0.0004,0.0005}}

gs =GridSearchCV(DecisionTreeClassifier(random_state=42),params, n_jobs=-1)

```

cv 매개변수는 기본이 5이므로, 
총 25개의 모델을 훈련한다.

이때 CPU 코어 수를 지정하는 것이 좋은데, default는 1이고, -1로 지정하면 시스템의 모든 코어를 사용하는 것이다.

학습
```python
gs.fit(train_input, train_target)

# 여기서 검증 점수가 가장 높게 나온 모델은 전체 훈련세트를 다시 학습하고, 이를 
#best_estimator_ 속성에 저장되어 있습니다.
dt = gs.best_estimator_
print(dt.score(train_input, train_target))
#0.961516

#GridSearch에서 찾은 최적의 매개변수는
#best_params_ 속성에 저장되어 있다.
print(gs.best_params_)
#{'min_impurity_decrease' : 0.0001}

#교차 검증의 평균 점수는 
#cv_results_ 에 저장되어 ㅇㅆ다.
print(gs.cv_results_['mean_test_score'])
# 교차검증의 평균점수를 list화 하여 출력한다.
#[점수1, 점수2,점수3,점수4,점수5]

# 열 추출
best_index = np.argmax(gs.cv_results_['mean_test_score'])
print(gs.cv_results_['params'][best_index])
#{'min_impurity_decrease' : 0.0001}
# 이는 best_params_와 동일하다.
```
best_estimator_

best_params_

## 하이퍼파라미터 튜닝 복수
```python
params = {'min_impurity_decrease' : np.arange(0.0001,0.001,0.0001),
'max_depth' : range(5,20,1),
'min_samples_split': range(2,100,10)
}

gs =GridSearchCV(DecisionTreeClassifier(random_state=42),params, n_jobs=-1)
gs.fit(train_input, train_target)
```

위에서처럼 정해진 범위가 아닐때 사용할 수 있는 방법은???

**Random Search**를 사용한다.

이는 값의 목록이 아닌, 매개변수를 샘플링할 수 있는 확률 분포 객체를 전달하는 것이다. 

사용법
1) 싸이파이에서 2개의 확률분포 클래스를 임포트하자.
    - scipy??
    - 파이썬의 라이브러리, 수치 계산 전용 라이브러이이다.
```python
from scipy.stats import uniform, randint

rgen= randint(0,10)
rgen.rvs(10)
# array([6,4,2,2,7,7,0,0,5,4])

np.unique(rgen.rvs(1000),return_counts=True)
#각각 나오는 값이 들어간 배열
#       나오는 값이 들어간 개수

# 실수
ugen = uniform(0,1)
ugen.rvs(10)
```

![alt text](image-50.png)

![alt text](image-51.png)

![alt text](image-52.png)

![alt text](image-53.png)

![alt text](image-54.png)

## 05-3 트리의 앙상블

**정형 데이터와 비정형 데이터**

정형 데이터(structured data) : 어떤 구조로 되어 있다는 것으로 

주로 CSV, 데이터베이스, 엑셀

비정형 데이터(unstructured data) : 위의 정형 데이터로 표현하기 어려운 데이터를 말한다.

책과 같은 텍스트 데이터, 사진, 음악 등이 있다.

**NoSQL 데이터 베이스**는 엑셀이나 CSV에 담기 어려운 텍스트나 JSON 데이터를 저장하는데 용이하다.

정형데이터의 GOAT 알고리즘은 **앙상블 학습(ensemble learning)**이다.

**앙상블**
- 이 알고리즘은 **결정트리 기반**이다.

비정형데이터는 무엇을 사용하니?

**신경망 알고리즘**이다.

## 앙상블 학습 알고리즘
정형 데이터의 GOAT 

**1. 랜덤 포레스트**

랜덤하게 결정트리들을 만들어서 다수의 결정트리를 만드는 것이다.

1. 각 트리당 훈련데이터 
    - 훈련 데이터에서 랜덤하게 샘플을 추출하여 훈련 데이터를 만든다.
    - 1000개에서 100개의 데이터를 중복조합 방식으로 추출하는 것이다.
    - 이를 **부트스트랩 샘플(bootstrap sample)**이라고 한다.
        - 이 샘플은 기본적으로 데이터 셋의 개수와 똑같기에 1000개로 한다.
2. 훈련
    - 노드 분할
    - 전체 특성 중에서 일부 특성을 무작위 고른 후
    - **최선의 분할**을 고른다.
    - RandomForestClassifier는 기본적으로 전체 특성 개수의 **제곱근 만큼**의 특성을 선택
3. 결과 도출
    - 분류
        - 각 트리의 클래스별 확률을 평균하여 가장 높은 확률을 가진 클래스를 예측한다.
    - 회귀
        - 단순히 각 트리의 예측을 평균합니다.    
4. 장점
    - 랜덤하게 선택한 샘플과 특성
        - 과대적합을 막기
        - 안정적인 성능을 얻을 수 있다.
5. 예제 코드
```python
# 데이터 불러오기
# pandas에서 numpy배열로 바꾸기
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
wine = pd.read_csv('https://bit.ly/wine-date')
data = wine[['alcohol', 'sugar','pH']].to_numpy()
target = wine['class'].to_numpy()
train_input, test_input, train_target, test_target = train_test_split(data, target, test_size=0.2, random_state=42)

# 교차 검증
# 랜덤포레스트
from sklearn.model_selection import cross_validate
from sklearn.ensemble import RandomForestClassifier
rf=RandomForestClassifier(n_jobs=-1,random_state=42)
scores = cross_validate(rf, train_input, train_target, return_train_score=True, n_jobs=-1)
print(np.mean(scores['train_score']), np.mean(scores['test_score']))
# 0.9973 0.8905
```

이는 과대적합이라고 판단할 수 있다.

이 예제는 매우 간단하고, 특성이 많지 않다.

그래서 그리드 서치를 사용하더라도 하이퍼파라미터 튜닝의 결과가 크게 나아지지 않는다.

랜덤 포레스트의 클래스의 매개변수는 DecisionTreeClassifier에서 제공하는 변수를 다 제공한다.

```python
rf.fit(train_input, train_target)
print(rf.feature_importances_)
#[0.231 0.500 0.267]

# 앞 절에서의 변수 중요도
# [0.12 0.86 0.007]
```

이는 과대한 집중을 피한다.

이는 더 많은 특성을 교육하는 기회를 제공한다.

이는 과대적합을 줄이고 일반화 성능을 높이는 데 도움이 된다.

**랜덤 포레스트의 또 다른 기능**

- 자체적인 모델 평가 기능
    - 데이터의 종류 : 선별 O : 부트스트랩, 선별 X : OOB
    - 사용 방식 : RandomForestClassifier 클래스의 oob_score 매개변수를 True로 지정하면 이용할 수 있다.
```python
rf = RandomForestClassifer(oob_score=True, n_jobs=-1, random_state=42)
rf.fit(train_input, train_target)
print(rf.oob_score_)
```

**2. 엑스트라 트리(Extra Trees)**

공통점

1) 기본적으로 100개의 결정 트리를 훈련한다.

2) 매개변수를 지원

3) 일부 특성을 랜덤하게 선택하여 노드를 분할

차이점

1) 부트스트랩을 사용하지 않는다.
- 전체 훈련 세트를 활용하여 트리를 만든다.

이는 가장 좋은 분할을 찾는 것이 아니라 **무작위**로 분할한다.

예제
```python
DecisionTreeClassifier(splitter='random')
#이를 통해 엑스트라 트리를 구현할 수 있다.
```
```python
from sklearn.ensemble import ExtraTreesClassifier
et = ExtraTreesClassifier(n_jobs=-1, random_state =42)
scores = cross_validate(et, train_input, train_target, return_train_score =True, n_jobs=-1)
print(np.mean(scores['train_score']),np.mean(scores['test_score']))
# 0.99 0.88
```

이 모델의 장점은 랜덤이기에 더 많은 결정 트리를 훈련해야 하지만, **빠른 계산 속도**가 장점이다.

```python
ef.fit(train_input, train_target)
print(ef.oob_score_)
```

**3. 그레디언트 부스팅(gradient boosting)**

깊이가 얕은 결정 트리를 사용하여 이전 트리의 오차를 보완하는 방식으로 앙상블 하는 방법

그래서 깊이가 3인 결정 트리를 100개 사용한다. 

장점 : 깊이가 얕은 결정트리는 과대적합에 강하고 높은 일반화 성능을 기대할 수 있다.

사용 방식

분류 : 로지스틱 손실 함수

회귀 : 평균 제곱 오차 함수

과정 설명
- 경사하강법에선 가장 낮은 곳으로 내려가기 위해 모델의 가중치와 절편을 조금씩 바꾸는 것으로 사용했다.
- 그레디언트 부스팅은 결정 트리를 계속 추가하면서 가장 낮은 곳으로 이동한다.
    - 깊이가 낮은 이유는 천천히 가야 하므로 이를 위해 **깊이가 낮은** 트리를 사용한다.

```python
from sklearn.ensemble import GradientBoositingClassifier
gb= GradientBoostingClassifier(random_state=42)
scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)
print(np.mean(scores['train_score']),np.mean(scores['test_score']))
#0.888 0.872
```

매개변수를 변화한 경우

n_estimators : 결정 트리 개수/ default : 100

learning_rate : 학습 속도 / default : 0.1

```python
gb= GradientBoostingClassifier(n_estimators =500,learning_rate=0.2, random_state=42)
scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)
print(np.mean(scores['train_score']),np.mean(scores['test_score']))
#0.94 0.87
```

```python
# 특성 중요도
rf.fit(train_input, train_target)
print(rf.feature_importances_)
#[0.158 0.680 0.161]
```

장점 : 
- 랜덤 포레스트 보다 정확도가 높다.
- 훈련 속도는 느리다.
- 즉, GradientBoostingClassifier에는 n_jobs 매개변수가 없다.
    - 순서가 정해진 상태로 추가되는 형태이기 때문이다.
- 회귀버전은 GradientBoostingRegressor가 있다.

이에 대해 속도와 성능을 개선한 것이 히스토그램 기반 그레디언트이다.

**히스토그램 기반 그레이디언트 부스팅(Histogram-based Gradient Boosting)**

특징
- 정형에선 가장 인기 있다.
- 입력 특성을 256개의 구간으로 나눈다.
    - 이를 통해 최적의 노드를 빠르게 찾을 수 있다.
- 전처리의 필요성??
    - 256개의 구간 중에서 하나를 떼어 놓고 누락된 값을 위해 사용하므로, 입력에 누락된 특성이 있더라도 *전처리는 필요하지 않다*.
```python
from sklearn.experimental import enable_hist_gradient_boosting
from sklearn.ensemble import HistGradientBoostingClassifier
hgb= HistGradientBoostingClassifier(random_state=42)
scores = cross_validate(hgb, train_input, train_target, return_train_score=True, n_jobs=-1)
print(np.mean(scores['train_score']),np.mean(scores['test_score']))
#0.93 0.88

#특성 중요도
hgb.fit(train_input, train_target)
print(hgb.feature_importances_)

#[0.23 0.500 0.267]

# 평가
hgb.score(test_input, test_target)
#0.872
```

- 회귀 버전은 HistGradientBoostingRegressor 클래스에 구현되어 있다.

**히스토를 구현한 라이브러리**

1. XGBoost 라이브러리
    - cross_validate()
    - tree_method 매개변수
        - 'hist'는 히스토그램 기반 그래디언트 부스팅 사용 가능
```python
from xgboost import XGBClassifier
xgb =XGBClassifier(tree_method='hist', random_state =42)
scores = cross_validate(xgb, train_input, train_target, return_train_score=True)
print(np.mean(scores['train_score']),np.mean(scores['test_score']))
#0.88 0.87
```
2. LightGBM
```python
from lightgbm import LGBMClassifier
lgb =LGBMClassifier( random_state =42)
scores = cross_validate(lgb, train_input, train_target, return_train_score=True)
print(np.mean(scores['train_score']),np.mean(scores['test_score']))
#0.933 0.878
```

![alt text](image-55.png)

# 플러스 알파
판다스 메소드
1. info() - 누락값
2. describe()
페이지 - p. 220 ~ 282
넘파이 메소드
1. shape => (1,2) : 

1: data object의 개수
2: 특성의 개수

coef_, intercept_

from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier(random_state=42)

그래프 표현
plot_tree()
from sklearn.tree import plot_tree
plt.figure(figsize=(10,7))
plot_tree(dt)
plt.show()

매개변수
max_depth : 1이면 leaf를 제외한 하나의 노드가 생긴다.
filled
feature_names : 특성의 이름을 전달하는 목적

그림 내에
feature_name
gini
samples
values


불순도
gini
entropy
가지치기

전처리 생략 가능


결정 트리 모델의 속성의 중요도를 찾는 방법
feature_importances_

cross validate()


최적값을 찾을 때 dict를 이용하는 방법
from sklearn.model_selection import GridSearchCV
params = {'min_impurity_decrease' : {0.0001,0.0002,0.0003,0.0004,0.0005}}

search에 대한 매개변수를 다시 찾아보기
~~~~~~
~~~~~~~
~~~~~~


# 따로 기록용

```python
