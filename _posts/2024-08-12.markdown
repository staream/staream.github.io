---
layout: post
title: 	"4th-week ML"
date: 	2024-07-26 15:52:17 +0900
categories: KhuDa
---

# 04 다양한 분류 알고리즘

# 04-1 로지스틱 회귀
## 1. 생각
- 해당 지점의 주변 이웃의 클래스에서 비율이 높은 클래스를 답으로 정하면 되지 않을까?
## 2. 데이터 준비하기
- CSV파일을 데이터 프레임으로 변환한 다음 head() 메서드로 처음 5개 행을 출력해 보자.
    ```python
    import pandas as pd
    fish = pd.read_csv('http://bit.ly/fis_csv')
    fish.head()
    ```
    - 데이터 프레임이란?

    1) 판다스에서 제공하는 2차원 표 형식이 데이터 구조.

    2) 넘파이처럼 열과 행으로 이루어져 있다.
    
    3) 통계와 그래프를 위한 메서드를 풍부하게 제공한다. 또 데이터프레임은 넘파이로 상호 변환이 쉽고, 사이킷런과 호환이 좋다.
- Species 열의 고유한 값을 추출하기 위해 **unique() of pandas**를 통해 클래스가 어떤 종류가 있는지 알아보자.
    ```python
    print(pd.unique(fish['Species']))
    # 반환 list
    ```
- Species를 제외한 열을 입력 데이터로 사용해보겠다. 열을 선택하는 방법은 아래와 같이 이용한다.
    ```python
    fish_input = fish[['Weight','Length','Diagonal','Height','Width']].to_numpy()
    # 넘파이 배열로 바꾸어 fish_input에 저장한다.
    
    #5개의 열 출력
    print(fish_input[:5]) 

    #target 준비
    fish_target= fish['Species'].to_numpy()

    #데이터 나누기
    from sklearn.model_selection import train_test_split
    train_input, test_input, train_target, test_target = train_test_split(fish_input, fish_target, random_state=42)

    # 표준화
    from sklearn.preprocessing import StandardScaler
    ss = StandardScaler()
    ss.fit(train_input)
    train_scaled= ss.transform(train_input)
    test_scaled= ss.transform(test_input)
    ```
## 3. K-최근접 이웃 분류기의 확률 예측
- 예제코드
    ```python
    from sklearn.neighbors import KNeighborsClassifier
    kn= KNeighborsClassifier(n_neighbors=3)
    kn.fit(train_scaled, train_target)
    print(kn.score(train_scaled,train_target))
    print(kn.score(test_scaled, test_target))
    #0.89
    #0.85
    # 과적합은 아니지만 과소적합

    # 정답 값을 출력
    print(kn.classes_)
    # 리스트 형태로 반환

    # 테스트에 있는 처음 5개의 예측 클래스를 출력해보자
    print(kn.predict(test_scaled[:5]))

    #확률은?
    import numpy as n
    # 각 클래스의 확률을 구하기
    proda = kn.predict_proba(test_scaled[:5])
    # 반환값 sample 개의 list가 있고, 그 안에 요소가 class 개수만큼 있다.


    # 각 확률을 소수점 4자리까지 구하는 걸로 (반올림)
    print(np.round(proda, decimals=4))
    # 순서는 classes_ 속성과 순서가 같다.
    # [0.  0.   0.6667  0.    0.3333  0.    0.]
    #위의 비율을 확인하기 위해 네 번째 샘플에 포함되는 거리와 index를 가져온다.
    distances, indexes = kn.kneighbors(test_scaled[3:4])
    # test_scaled[3:4] 인 4번째 sample의 인접 input을 알아내기 위해 위와 같은 코드를 작성했다.
    # 이때는 해당 매서드 안에는 이중배열이 들어가야 함으로 indexing하여 넣는다.
    print(train_target[indexes])
    # 출력 결과는 n_neighbors가 3이므로, 0,1/3,2/3,3/3이 전부이다.
    ```
- 한계점
    - 위의 코드에 대한 결과는 **이산적인** 값만을 준다.
## 4. Logistic regression
- 로지스틱 회귀 : 회귀면서 분류모델이다.
- 특징
    1) 선형회귀처럼 **선형 방정식**을 학습한다.

    ![alt text](image-31.png)
    
    - 각 abcde는 가중치 혹은 계수
    - z를 확률로 표현하기 위해선 **0~1** 해야 한다.
        - **sigmoid function(= logistic function)**를 이용한다.

        ![alt text](image-32.png)

        - 이때 z를 sigmoid function에 넣는 것이다.
        - 이에 대한 결과는 아래와 같은 코드로 나온다.
    ```python
    import numpy as np
    import matplotlib.pyplot as plt
    # -5와 5 사이를 0.1 간격으로 배열 z를 만든다.
    z = np.arange(-5,5,0.1)
    phi = 1/ (1+np.exp(-z))
    # x축과 y축에 대한 값
    plt.plot(z, phi)
    plt.show()
    ```
    ![alt text](image-33.png)
    
    - 이때 기준은 0.5로 잡고 있다.
## 5. 로지스틱 회귀를 이용한 이진 분류
- **Boolean Indexing** : 넘파이 배열은 True, False 값을 전달하여 행을 선택할 수 있는 기술
    - 예시 : 여러 클래스 중에서 특정 클래스를 가진 sample을 얻으려고 할 때에는 **Boolean Indexing**을 사용한다.
    1) list indexing
    ```python
    # A, C를 뽑아야 하는 경우
    char_arr = np.array(['A','B','C','D','E'])
    print(char_arr[[True,False,True,False,False]])
    # 출력 : ['A','C']
    ```
    2) using comparison operator
    ```python
    # data_selection by using comparison operator
    A_B_indexes = (train_target == 'A')| (train_target =='B')
    train_A_B = train_scaled[A_B_indexes]
    test_A_B = train_scaled[A_B_indexes]
    ```
- 모델 학습
    ```python
    from sklearn.linear_model import LogisticRegression
    lr = LogisticRegression()
    lr.fit(train_A_B,target_A_B)

    # train_A_B에 있는 처음 5개 샘플을 예측해보자.
    print(lr.predict([train_A_B[:5]]))
    ```
    ![alt text](image-35.png)

    위의 그림처럼 2개의 확률이 출력되었다. 첫 번째 열이 음성클래스(0)에 대한 확률이다. 2번째는 양성클래스(1)에 대한 확률. 
    <tr>
    이에 대한 순서는??
    <tr>

    lr.classes_ 에서 나온 list의 순서가 위의 확률에 해당하는 클래스의 순서와 맞다

- 모델 분석
    ```python
    # 각 가중치 출력
    print(lr.coef_,lr.intercept_)
    ```
- z값을 얻는 법
    ```python
    decisions = lr.decision_function(train_A_B[:5])
    print(decisions)
    # 5개의 샘플에 대한 z값을 list로 놓았다.
    ```
- sigmoid에 대입하는 법
    ```python
    #1.
    phi = 1/ (1+np.exp(-z))
    #2.
    from scipy.special import expit
    print(expit(decisions))
    # z를 넣어 나온 확률값
    ```
## 6. 로지스틱 회귀로 다중 분류 수행하기
1) LogisticRegression class 이용
2) LogisticRegression class는 기본적으로 **반복적인 알고리즘**을 사용
    - max_iter 매개변수는 기본값 100이다. 준비한 데이터셋으로 학습을 진행하면 **반복학습**이 부족하다고 경고가 발생. 1000으로 늘린다.
3) LogisticRegression class는 기본적으로 릿지 회귀와 같이 계수의 제곱을 규제한다.
    - L2 규제 사용
    - lidge의 alpha 매개변수는 규제의 양을 조절한다.
        - 클수록 규제가 커짐
    - LogisticRegression에서 C매개변수는 규제를 제어하는 매개변수이다.
        - 작을수록 규제가 커짐
        - 기본값 1
        - 완화를 위해 20 입력
4) 데이터는 여러 클래스가 있는 train_scaled와 train_target을 이용한다.
    ```python
    lr = LogisticRegression(C=20,max_iter = 1000)
    lr.fit(train_scaled, train_target)
    print(lr.score(train_scaled, train_target))
    print(lr.score(test_scaled, test_target))
    #0.9377
    #0.925
    # 괜찮은 결과를 가지고 있다.\

    # 먼저 있는 5개의 예측 출력
    print(lr.predict(test_scaled[:5]))

    # sample이 예측한 확률 구하기
    proba = lr.predict_proba(test_scaled[:5])
    print(np.round(proda,dcimal=3))
    # 5개의 list를 묶은 배열을 출력하는데 값은 3번째까지 구한다. 반올림한다.

    #classes 속성을 출력
    print(lr.classes_)

    # 가중치/ 계수 구하기
    print(lr.coef_.shape, lr.intercept_.shape)
    # (7,5), (7,)
    ```
    - 이진 분류를 할 때에는 (1,5), (1,)으로 나왔었다.
    - 다중 분류에선 이 계산을 7번 함으로써 이중에서 가장 확률이 높게 나온 것을 7개의 클래스중에서 하나라고 예측한다.
        - **소프트맥스**함수를 사용하여 7개의 z값을 확률로 변환한다.
    - soft-max vs sigmoid
        - 0~1의 값으로 변환하는 게 다중인지 이진인지를 판단하는 것
        - 소프트 맥스는 여러개의 값을 0~1로 압축하고 이들의 합을 1이 되도록 만드는 과정을 거친다.
        - 다른 말로 **정규화된 지수함수**라고 부른다.
        ![alt text](image-38.png)

    ```python
    # z값 구하기
    decision = lr.decision_function(test_scaled[:5])
    print(np.round(decision, decimals=2))


    from scipy.special import softmax
    proba = softmax(decision, axis=1)
    print(np.round(proda, decimals=3))
    ```

    1) softmax()
    - axis 매개변수는 소프트맥스를 계산할 축을 지정합니다. 
        - axis=1이면 각 행, 즉 각 샘플에 대해 소프트맥스를 계산합니다. 
# 04-2 확률적 경사 하강법
## 1. 점진적인 학습
- 이유 : 새로운 모델을 만드는 것은 지속적으로 나오는 데이터에 대해 학습하는 속도가 점점 느려질 것이다.
- 방안 : 이에 대해 우리는 모델은 유지하면서 새로운 데이터에 대해 학습을 **추가적으로** 진행하는 방안을 생각했다.
- 용어
    1) 학습 방식 : 점진적 학습/ 온라인 학습
    2) 알고리즘 : 확률적 경사 하강법(Stochastic Gradient Descent)
## 2. 확률적 경사 하강법
- 확률적 : '무작위하게'
- 경사 : 기울기
- 하강법 : 내려가는 방법
- 뜻 : 가장 가파른 경사를 따라 원하는 지점에 도달하는 것이 목표이다.
- 훈련 : 내려오는 과정이 경사 하강법 모델의 훈련이다.
- 내려올 때 가장 가파른 길을 찾는 방법은 무엇이니? 
    - 훈련 세트를 사용해 모델을 훈련하기에 이 세트를 사용하여 가장 가파른 길을 찾게 되는 것이다.
    
    이처럼 훈련세트에서 랜덤하게 하나의 샘플을 고르는 것이 **확률적 경사 하강법**이다.

    그럼 완주를 못할 시에는 어떻게 하니?

    다시 시작하는 것이다. 
    
    - 훈련세트에 모든 샘플을 다시 넣는다.

    다시 랜덤하게 경사를 내려간다.

    이처럼 훈련세트를 한 번 모두 사용하는 과정을 **epoch**라고 한다.

    다른 방법인 몇 개의 샘플을 선택해서 경사를 따라 내려가는 것을 **minibatch gradient descent**라고 한다.

    극단적으로 한 번 경사로를 따라 이동하기 위해 **전체 샘플**을 사용하는  경우가 있는데, 이를 **batch gradient descent**라고 한다.

    배치 경사 하강법은 컴퓨터 자원이 요구하기에 이에 대해 생각해야 한다.

    % 신경망은 batch가 아닌 minibatch, stochastic으로 이용된다.

## 3. 손실함수
이 수치는 얼마나 오차가 큰지를 나타낸다.

어떤 값이 최솟값인지는 알지 못한다.

만족할 수준으로 갔을 때 인정해야 한다.

% 비용 함수(cost function)는 훈련 세트에 있는 모든 샘플에 대한 손실 함수의 합을 말한다.

% 손실 함수는 샘플 하나에 대한 손실을 정의한다.

분류에서 정확도는 -1->0으로 가는 것이 좋지 않을까?

분류에서의 정확도에는 문제가 있다.

가능한 값이 연속적이지 않다는 점이다.

이는 조금씩 움직이는 경사하강법에서 힘들 것으로 생각된다.

이를 어떻게 해결해야 할까?

사용할 수 있는 방법은 logistic regression에서 사용한 방법을 다시 이용하면 된다. 

## 4. logistic cost function
1. 이진을 기준으로

양성에 대한 확률과 양성 값(=1)을 곱해준다.

거기에 -1을 곱한다면 -1~0까지로 확률이 나올 것이다. -1은 가장 낮은 확률, 0은 가장 높은 확률이 된다.

이에 대해 다른 방법은 log를 취하는 방법이다.

-----

타깃이 1

log(predict_probability)

타깃이 0

log(1 - predict_probability)

-----

위의 손실함수를 **logistic loss function** or **binary cross-entropy loss function**이라고 한다.

2. 다중인 경우

이에 대한 손실함수는 **cross-entropy loss function**이라고 한다.

3. 회귀에 사용되는 cost function

**절댓값 오차**를 사용할 수 있다.

타깃에서 예측을 뺀 값을 제곱한 다음 모든 샘플에 더한 후에 평균한 값이다.

이 값은 작을 수록 좋다.

## 5.예제 코드
SGDClassifier

1. 판다스 데이터프레임 만들기

```python
import pandas as pd   
fish = pd.read_csv('https://bit.ly/fish_csv') 
```
target : Species
input : others

```python
fish_input = fish[['Weight', 'Length'. 'Diagonal', 'Height', 'Width']].to_numpy()
fish_target = fish['Species'].to_numpy()   
```

2. 세트를 나누기

```python
from sklearn.model_selection import train_test_split
train_input, test_input, train_target, test_target =train_test_split(fish_input, fish_target, random_state=42)
```

3. 표준화 전처리

```python
from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
ss.fit(train_input)
train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)    
```

4. 확률적 경사 하강법을 제공하는 **분류용 클래스**

SGDClassifier

매개변수 

1) loss : 'log'는 로지스틱 손실 함수
2) max_iter : 수행할 에포크 횟수

```python
from sklearn.linear_model import SGDClassifier 
sc= SGDClassifier(loss='log', max_iter=10, random_state=42)
sc.fit(train_scaled, train_target)
print(sc.score(train_scaled, train_target))
print(sc.score(test_scaled, test_target))
#0.773
#0.775
```

정확도가 낮으니, 반복횟수를 늘려준다.

5. 추가 학습

사용 메소드 : partial_fit

```python
sc.partial_fit(train_scaled, train_target)
print(sc.score(train_scaled, train_target))  
print(sc.score(test_scaled, test_target))
#0.81
#0.825
```

여기서 사용하는 SGDClassifier는 미니배치 경사 하강법이나 배치 하강법을 제공하지 않는다. 하지만 **신경망**에선 미니배치 경사 하강법을 사용해본다.

## 6. 에포크와 과대/과소적합

조기 종류 : 과대적합이 시작하기 전에 훈련을 멈추는 것

사용 메소드 : fit이 아닌 partial_fit을 사용하려고 한다.

1. class 구현

partial을 사용하기 위해선 메서드에 훈련 세트에 있는 전체 클래스의 레이블을 전달해야 한다. 

이를 위해 np.unique() 함수로 train_target에 있는 7개 생선의 목록을 만든다.

```python
import numpy as np
sc = SGDClassifier(loss='log', random_state=42)
train_score =[]
test_score = []
classes = np.unique(train_target)
```

2. 훈련 하기

300번의 에포크 동안 훈련을 반복하여 진행하고, 반복마다 훈련세트와 테스트 세트의 점수를 계산하여 list에 넣기

```python
for _ in range(0,300):
    sc.partial_fit(train_scaled,train_scaled, classes= classes)
    train_score.append(sc.score(train_scaled, train_target))
    test_score.append(sc.score(test_scaled, test_target))
```

3. 시각화

```python
import matplotlib.pyplot as plt
plt.plot(train_score)
plt.plot(test_score)
plt.show()
```

4. 모델 훈련 (최적의 매겨변수)

tol 매개변수에는 향상될 최솟값을 지정한다.

tol 매개변수는 None으로 지정하여 자동으로 멈추지 않고, 100만큼 무조건 반복한다.

```python
from sklearn.linear_model import SGDClassifier 
sc= SGDClassifier(loss='log', max_iter=100,tol=None, random_state=42)
sc.fit(train_scaled, train_target)
print(sc.score(train_scaled, train_target))
print(sc.score(test_scaled, test_target))
#0.957
#0.925
```

loss 매개변수 : 기본값은 **hinge**이다.

**hinge loss** is used for **support vector machine algorithm**

```python
from sklearn.linear_model import SGDClassifier 
sc= SGDClassifier(loss='hinge', max_iter=100,tol=None, random_state=42)
sc.fit(train_scaled, train_target)
print(sc.score(train_scaled, train_target))
print(sc.score(test_scaled, test_target))
#0.957
#0.925
```